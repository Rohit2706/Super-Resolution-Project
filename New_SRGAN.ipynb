{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_SRGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FdeSzFoeu62y"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1lQ1-y5u-TH"},"source":["# imports\n","import torch \n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import PIL\n","from PIL import Image\n","\n","import h5py\n","import sys, os, math,random,time\n","from datetime import datetime\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","!pip install icecream\n","from icecream import ic \n","\n","# !pip install ipyplot\n","# import ipyplot\n","\n","from IPython.display import clear_output "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tnly0fQvEox"},"source":["# tensorboard writer instantiation \n","writer = SummaryWriter()\n","# global variables declaration \n","device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","dataset_dir=\"<PATH TO DIV2k Data>\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBGjbbXPvJg0"},"source":["def get_patch(lr_img, hr_img, patch_size=96, scale = 4 , ix=-1, iy=-1):\n","    (ih, iw) = lr_img.shape[0],lr_img.shape[1]\n","    patch_size=96\n","    scale=4\n","    ip = patch_size\n","    tp = patch_size *scale\n","\n","    ix = random.randrange( 0 , iw - patch_size - 1 )\n","    iy = random.randrange( 0 , ih - patch_size - 1 )\n","\n","    (tx, ty) = (scale * ix, scale * iy)\n","\n","    lr_patch = lr_img[iy : iy + ip, ix : ix + ip,:]\n","    hr_patch = hr_img[ty : ty + tp, tx : tx + tp,:]\n","\n","    return lr_patch,hr_patch "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuf4EZMBvM94"},"source":["class DIV2KDataset(Dataset):\n","    def __init__(self,dataset_dir,dataset_length,scale=4,patch_size=96):\n","        self.lr_h5_file = h5py.File(f'{dataset_dir}/DIV2K_CAR_LR_X4_train.h5' , 'r')['X4']\n","        self.hr_h5_file = h5py.File(f'{dataset_dir}/DIV2K_HR_train.h5' , 'r')['HR']\n","\n","        self.dataset_dir=dataset_dir\n","        self.dataset_length=dataset_length\n","        self.patch_size=patch_size\n","        self.scale=scale\n","\n","    def __len__(self):\n","        return self.dataset_length\n","    \n","    def __getitem__(self,idx):\n","        img_name =str(idx)\n","\n","        lr_img = self.lr_h5_file[img_name][()]\n","        hr_img = self.hr_h5_file[img_name][()]\n","\n","        lr_patch,hr_patch=get_patch(lr_img,hr_img,self.scale,self.patch_size)\n","        \n","        return {\n","            'lr_img':torch.from_numpy(lr_patch),\n","            'hr_img':torch.from_numpy(hr_patch),\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yez0qaXCvZ0t"},"source":["div2k_dataset = DIV2KDataset(dataset_dir=dataset_dir,dataset_length=800,scale=4)\n","dataloader = DataLoader(div2k_dataset, batch_size=8,shuffle=True,pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luc3t8rxvaUI"},"source":["# check if the loader is working properly \n","patches=div2k_dataset[1]\n","display(Image.fromarray(patches['lr_img'].numpy(),mode=\"RGB\"))\n","display(Image.fromarray(patches['hr_img'].numpy(),mode=\"RGB\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sk5VakVSvnLo"},"source":["class VGGLoss(torch.nn.Module):\n","    def __init__(self, feature_layer= 35):\n","        super(VGGLoss, self).__init__()\n","        model = torchvision.models.vgg19(pretrained=True)\n","        self.features = torch.nn.Sequential(*list(model.features.children())[:feature_layer]).eval()\n","        # Freezing parameters, not to train.\n","        for name, param in self.features.named_parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, source, target) -> torch.Tensor:\n","        vgg_loss = torch.nn.functional.l1_loss(self.features(source), self.features(target))\n","        return vgg_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4zLxNbhvzvV"},"source":["def default_conv(in_channels, out_channels, kernel_size, bias=True):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size,padding=(kernel_size//2), bias=bias)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27uOHVruv2QE"},"source":["# Normalization Module\n","class MeanShift(nn.Conv2d):\n","    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n","        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n","        std = torch.Tensor(rgb_std)\n","        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n","        self.weight.data.div_(std.view(3, 1, 1, 1))\n","        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n","        self.bias.data.div_(std)\n","        self.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9iqkR33v6zW"},"source":["# Residual Block Module\n","class ResBlock(nn.Module):\n","    def __init__(\n","        self, conv, n_feat, kernel_size,\n","        bias=True, bn=False, act=nn.ReLU(True)):\n","\n","        super(ResBlock, self).__init__()\n","        m = []\n","        for i in range(2):\n","            m.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n","            if bn: m.append(nn.BatchNorm2d(n_feat))\n","            if i == 0: m.append(act)\n","\n","        self.body = nn.Sequential(*m)\n","\n","    def forward(self, x):\n","        res = self.body(x).mul(1)\n","        res += x\n","\n","        return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPs2QPZLv6Ka"},"source":["# Self Attention Module\n","class SelfAttentionBlock(nn.Module):\n","    def __init__(self,n_feats):\n","        super(SelfAttentionBlock, self).__init__()\n","        self.X_copy_1 = None\n","        self.X_copy_2 = None\n","        self.conv_layer1 = default_conv(n_feats,n_feats,1)\n","        self.conv_layer2 = default_conv(n_feats,n_feats,1)\n","        self.conv_layer3 = default_conv(n_feats,n_feats,1)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, X):\n","        self.X_copy_1 = torch.clone(X)\n","        self.X_copy_2 = torch.clone(X)\n","\n","        layer1 = self.conv_layer1(self.X_copy_1)\n","        layer2 = self.conv_layer2(self.X_copy_2)\n","        projection_map = torch.matmul(layer1,layer2.transpose(2,3))\n","        attention_map = self.sig(projection_map)\n","        sa_output = torch.matmul(attention_map,self.conv_layer3(X))\n","        return sa_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQQQdjsmwPfc"},"source":["# Channel Attention \n","class CALayer(nn.Module):\n","    def __init__(self, channel, reduction=16):\n","        super(CALayer, self).__init__()\n","        self.channel=channel\n","        self.reduction=reduction\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        # self.conv_du = nn.Sequential(\n","        #         nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n","        #         nn.ReLU(inplace=True),\n","        #         nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n","        #         nn.Sigmoid()\n","        # )\n","        self.conv_du = nn.TransformerEncoderLayer(d_model=channel, nhead = reduction, dim_feedforward = channel//reduction , dropout=0, activation='gelu')\n","    def forward(self, x):\n","        y = self.avg_pool(x)\n","        y = self.conv_du(y.reshape(1,-1,self.channel))\n","        return x * y.reshape(-1,self.channel,1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01vPl0UAxP-c"},"source":["# Residual Channel Attention Block (RCAB)\n","class RCAB(nn.Module):\n","    def __init__(self, conv, n_feat, kernel_size, reduction,bias=True, act=nn.ReLU(True)):\n","        super(RCAB, self).__init__()\n","        modules_body = []\n","        for i in range(2):\n","            modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n","            if i == 0: modules_body.append(act)\n","        modules_body.append(CALayer(n_feat, reduction))\n","        self.body = nn.Sequential(*modules_body)\n","\n","    def forward(self, x):\n","        res = self.body(x)\n","        res += x\n","        return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9eZMAwTxkFY"},"source":["# Residual Group (RG)\n","class ResidualGroup(nn.Module):\n","    def __init__(self, conv, n_feat, kernel_size, reduction, act, n_resblocks):\n","        super(ResidualGroup, self).__init__()\n","        modules_body = []\n","        modules_body = [RCAB(conv, n_feat, kernel_size, reduction, bias=True, act=nn.ReLU(True)) for _ in range(n_resblocks)]\n","        modules_body.append(conv(n_feat, n_feat, kernel_size))\n","        self.body = nn.Sequential(*modules_body)\n","\n","    def forward(self, x):\n","        res = self.body(x)\n","        res += x\n","        return res   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tv_iIj5RwLVp"},"source":["# Upsampling Module\n","class Upsampler(nn.Sequential):\n","    def __init__(self, conv, scale, n_feat, bn=False, act=False, bias=True):\n","        m = []\n","        if (scale & (scale - 1)) == 0:    # Is scale = 2^n?\n","            for _ in range(int(math.log(scale, 2))):\n","                m.append(conv(n_feat, 4 * n_feat, 3, bias))\n","                m.append(nn.PixelShuffle(2))\n","                if bn: m.append(nn.BatchNorm2d(n_feat))\n","                if act: m.append(act())\n","        elif scale == 3:\n","            m.append(conv(n_feat, 9 * n_feat, 3, bias))\n","            m.append(nn.PixelShuffle(3))\n","            if bn: m.append(nn.BatchNorm2d(n_feat))\n","            if act: m.append(act())\n","        else:\n","            print(\"Scale not implemented\")\n","        super(Upsampler, self).__init__(*m)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EV4ZmqeLxqf_"},"source":["# GAN Generator\n","class SRGenerator(nn.Module):\n","    def __init__(self, conv=default_conv):\n","        super(SRGenerator, self).__init__()\n","        \n","        n_resgroups = 2\n","        n_resblocks = 4\n","        n_feats = 128\n","        kernel_size = 3\n","        reduction = 16\n","        scale = 4\n","        act = nn.ReLU(True)\n","        n_rec_blocks = 4\n","        \n","        self.small_kernel_size=3\n","        self.large_kernel_size=5\n","        self.kernel_sizes=2  \n","\n","        # RGB mean for DIV2K\n","        rgb_mean = (0.4488, 0.4371, 0.4040)\n","        rgb_std = (1.0, 1.0, 1.0)\n","        self.sub_mean = MeanShift(1, rgb_mean, rgb_std)\n","\n","        self.feature_small = default_conv(3,n_feats,self.small_kernel_size)\n","        self.sa_small_1 = SelfAttentionBlock(n_feats=n_feats)\n","        self.sa_small_2 = SelfAttentionBlock(n_feats=n_feats)\n","\n","        self.feature_large = default_conv(3,n_feats,self.large_kernel_size)\n","        self.sa_large_1 = SelfAttentionBlock(n_feats=n_feats)\n","        self.sa_large_2 = SelfAttentionBlock(n_feats=n_feats)\n","\n","\n","        # define body module\n","        modules_body_1 = [ResidualGroup(conv, n_feats, kernel_size, reduction, act=act, n_resblocks=n_resblocks) for _ in range(n_resgroups)]\n","        modules_body_2 = [ResidualGroup(conv, n_feats, kernel_size, reduction, act=act, n_resblocks=n_resblocks) for _ in range(n_resgroups)]\n","\n","        modules_body_1.append(conv(n_feats, n_feats, kernel_size))\n","        modules_body_2.append(conv(n_feats, n_feats, kernel_size))\n","\n","        conv_1x1 = default_conv(n_feats*self.kernel_sizes,n_feats*self.kernel_sizes,kernel_size=1)\n","        ca = CALayer(n_feats*self.kernel_sizes)\n","        self.rec_input=nn.Sequential(*[conv_1x1,ca])\n","\n","        # define tail module\n","        modules_tail = [Upsampler(conv, scale, n_feats*self.kernel_sizes, act=False),conv(n_feats*self.kernel_sizes, 3, kernel_size)]\n","\n","        self.add_mean = MeanShift(1, rgb_mean, rgb_std, 1)\n","\n","        # self.head = nn.Sequential(*modules_head)\n","        self.res_body_1 = nn.Sequential(*modules_body_1)\n","        self.res_body_2 = nn.Sequential(*modules_body_2)\n","        self.tail = nn.Sequential(*modules_tail)\n","\n","    # for kernel_size 3        \n","    def small_proc(self, X):\n","\n","        features_ext = self.feature_small(X)\n","        # sa11_features=self.sa_small_1(features_ext)\n","        resblocks_out = self.res_body_1(features_ext)\n","        resblocks_out+=features_ext\n","\n","        return resblocks_out#self.sa_small_2() \n","    \n","    # for kernel_size 9\n","    def large_proc(self, X):\n","        \n","        features_ext = self.feature_large(X)\n","        # sa21_features=self.sa_large_1(features_ext)\n","        resblocks_out = self.res_body_2(features_ext)\n","        resblocks_out+=features_ext\n","        \n","        return resblocks_out#self.sa_large_2() \n","\n","    def forward(self, x):\n","        x = self.sub_mean(x)\n","        x_copy = torch.clone(x)\n","        \n","        #get each kernel's output\n","        small_Y = self.small_proc(x)\n","        large_Y = self.large_proc(x)  \n","\n","        concat_output=torch.cat((small_Y,large_Y),1) \n","        rec_input=self.rec_input(concat_output)\n","\n","        x = self.tail(rec_input)\n","        x = self.add_mean(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOTM9xAoxuZu"},"source":["# PatchGAN discriminator\n","# Snippet borrowed from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/networks.py#L538\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n","        super(NLayerDiscriminator, self).__init__()\n","        use_bias = True\n","        kw = 4\n","        padw = 1\n","        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n","        nf_mult = 1\n","        nf_mult_prev = 1\n","        for n in range(1, n_layers):  # gradually increase the number of filters\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2 ** n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2 ** n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, x):\n","        return self.model(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZfs2t6GysZW"},"source":["gen_model = SRGenerator().to(device)\n","disc_model = NLayerDiscriminator().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQYF1nMdy29i"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print('gen_model',f'{count_parameters(gen_model):,}')\n","print('disc_model',f'{count_parameters(disc_model):,}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzIsldYoy9Dk"},"source":["vgg_loss_criterion=VGGLoss().to(device)\n","l1_loss_criterion=nn.L1Loss()\n","adv_criterion=nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLQBoun3y_DT"},"source":["gen_optimizer = torch.optim.Adam(gen_model.parameters(), lr=0.00015) \n","gen_scheduler = torch.optim.lr_scheduler.StepLR(gen_optimizer, step_size=125, gamma=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OBQQDqRfzABQ"},"source":["disc_optimizer = torch.optim.Adam(disc_model.parameters(), lr=0.0001) #lr=0.00002\n","disc_scheduler = torch.optim.lr_scheduler.StepLR(disc_optimizer, step_size=125, gamma=0.9) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVU6V-atzEWH"},"source":["GEN_MODEL_PATH=f'/content/drive/MyDrive/trained_models/test_model_GEN_{datetime.now()}.pt'\n","DISC_MODEL_PATH=f'/content/drive/MyDrive/trained_models/test_model_DISC_{datetime.now()}.pt'\n","# GEN_MODEL_PATH=f'/content/drive/MyDrive/trained_models/test_model_GEN_2021-04-24 19_26_37.116762.pt'\n","# DISC_MODEL_PATH=f'/content/drive/MyDrive/trained_models/test_model_DISC_2021-04-24 19_26_37.116821.pt'\n","load_model=False\n","save_model=True\n","if load_model:\n","    gen_model.load_state_dict(torch.load(GEN_MODEL_PATH,map_location=device))\n","    disc_model.load_state_dict(torch.load(DISC_MODEL_PATH,map_location=device))\n","print(GEN_MODEL_PATH)\n","print(DISC_MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hedcZhu8Wg0L"},"source":["epochs=200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4mnKI39zIjl"},"source":["if save_model: \n","    for epoch in tqdm(range(epochs)):\n","        for k,data in enumerate(dataloader):\n","            \n","            # Batching\n","            lr_batch = (data['lr_img']).permute(0, 3,  1, 2).to(device).float()\n","            hr_batch = (data['hr_img']).permute(0, 3,  1, 2).to(device).float()\n","            batch_size = lr_batch.shape\n","\n","            # The real sample label is 1, and the generated sample label is 0.\n","            tensor_shape = (batch_size[0], 1, 46, 46 )     # for 96 X 96 patch size; generalize it later\n","            real_label = torch.full(tensor_shape, 1, dtype=lr_batch.dtype).to(device)\n","            fake_label = torch.full(tensor_shape, 0, dtype=lr_batch.dtype).to(device)\n","\n","            # Set discriminator gradients to zero.\n","            disc_model.zero_grad()\n","\n","            # Feed forward in Discriminator\n","            real_output = disc_model(hr_batch)\n","            sr = gen_model(lr_batch)\n","            fake_output = disc_model(sr.detach())\n","\n","            # Adversarial loss for real and fake images (relativistic average GAN)\n","            d_loss_real = adv_criterion(real_output - torch.mean(fake_output), real_label)\n","            d_loss_fake = adv_criterion(fake_output - torch.mean(real_output), fake_label)\n","            disc_loss =(d_loss_real + d_loss_fake) / 2 \n","\n","            # Discriminator backward pass\n","            disc_loss.backward()\n","\n","            # Update discriminator params\n","            disc_optimizer.step()\n","\n","            print(\"[EPOCH]: %i, [BATCH]: %i [DISC_LOSS]: %.6f\" % (epoch,k, disc_loss.item()))\n","            \n","            # Set generator gradients to zero.\n","            gen_optimizer.zero_grad()\n","\n","            #  VGG19_loss\n","            content_loss = vgg_loss_criterion(sr, hr_batch)\n","\n","            # pixel-wise L1 loss\n","            pixel_loss = l1_loss_criterion(sr, hr_batch)\n","\n","            # The accuracy probability of high resolution image and super-resolution image is calculated without calculating high-resolution gradient.\n","            real_output = disc_model(hr_batch)  # No train real fake image.\n","            fake_output = disc_model(sr)  # Train fake image.\n","            \n","            # Adversarial loss (relativistic average GAN)\n","            adversarial_loss = adv_criterion(fake_output - torch.mean(real_output), real_label)\n","\n","            g_loss = content_loss + 0.005* adversarial_loss + 0.01*pixel_loss # in next test change to: content_loss + 0.01*adversarial_loss + 0.1*pixel_loss\n","        \n","            print(\"[EPOCH]: %i, [BATCH]: %i [GEN_LOSS]: %.6f\" % (epoch,k, g_loss.item()))\n","            print(\"content_loss:\",content_loss.item(),\"pixel_loss\",pixel_loss.item(),\"adversarial_loss\",adversarial_loss.item())\n","\n","            # Generator backward pass \n","            g_loss.backward()\n","\n","\n","            # Update generator params\n","            gen_optimizer.step()\n","\n","            # write values to tensorboard\n","            writer.add_scalar('Loss/g_loss', g_loss.item(), 100*epoch + k )\n","            writer.add_scalar('Loss/d_loss', disc_loss.item(), 100*epoch + k )\n","            writer.add_scalar('Loss/content_loss', content_loss.item(), 100*epoch + k )\n","            writer.add_scalar('Loss/pixel_loss', pixel_loss.item(), 100*epoch + k )\n","            writer.add_scalar('Loss/adversarial_loss', adversarial_loss.item(), 100*epoch + k )\n","\n","            if not k%10:\n","                a=(hr_batch).permute(0, 2, 3, 1).cpu().numpy().astype(np.int8)[0]\n","\n","                display(PIL.Image.fromarray( data['hr_img'].numpy().astype(np.int8)[0],mode=\"RGB\"))\n","\n","                show_sr=(sr).permute(0, 2, 3, 1).cpu().detach().numpy().astype(np.int8)[0]\n","                print(show_sr.shape)\n","                display(PIL.Image.fromarray( show_sr,mode=\"RGB\"))\n","\n","                # writer.add_images(str(100*epoch + k),(sr).permute(0, 2, 3, 1),dataformats='NHWC')\n","                \n","                torch.save(gen_model.state_dict(),GEN_MODEL_PATH)\n","                torch.save(disc_model.state_dict(),DISC_MODEL_PATH)\n","                print(\"saved!\")\n","\n","            gen_scheduler.step()    \n","            disc_scheduler.step()    \n","            print(\"\")\n","        if epoch!=epochs-1:\n","            clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eE9_s6vHzMVE"},"source":["# Test image dataloader\n","test_dataset = DIV2KDataset(dataset_dir=dataset_dir,dataset_length=800,scale=4)\n","test_loader = DataLoader(test_dataset, batch_size=1,shuffle=True,pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka16ceLxzPrJ"},"source":["def test_model():\n","    test_dataset=next(iter(test_loader))\n","    test_lr_dataset_tensor=test_dataset['lr_img']\n","    display(PIL.Image.fromarray(test_lr_dataset_tensor[0].numpy(), mode=\"RGB\"))\n","\n","    test_lr_dataset_tensor=test_lr_dataset_tensor.permute(0, 3,  1, 2).to(device).float()\n","    test_hr_dataset_tensor=test_dataset['hr_img']\n","\n","    sr_imgs=gen_model(test_lr_dataset_tensor.float())\n","    show_sr_img=(sr_imgs).permute(0, 2, 3, 1).cpu().detach().numpy().astype(np.int8)[0]\n","    print(show_sr_img.shape)\n","\n","    display(PIL.Image.fromarray( show_sr_img,mode=\"RGB\"))    \n","    display(PIL.Image.fromarray(test_hr_dataset_tensor[0].numpy(), mode=\"RGB\"))\n","    \n","    return (test_lr_dataset_tensor,test_hr_dataset_tensor,sr_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_bIq0BszVmE"},"source":["psnrs=[]\n","for i in range(10):\n","    lr, hr, sr = test_model()\n","    psnrs.append(cv2.PSNR(sr.permute(0, 2,  3, 1)[0].cpu().detach().numpy().astype(np.int8)[0],hr[0].cpu().detach().numpy().astype(np.int8)[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kL4UcPBKzXwY"},"source":["psnrs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2I1z-KdzahB"},"source":["# writer.close()\n","%load_ext tensorboard\n","# %reload_ext tensorboard\n","%tensorboard --logdir \"/content/runs\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWlivw18gLt5"},"source":[""],"execution_count":null,"outputs":[]}]}